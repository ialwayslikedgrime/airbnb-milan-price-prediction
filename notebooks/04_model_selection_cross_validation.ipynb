{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **04 MODELING (NESTED CROSS-VALIDATION)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns  \n",
    "\n",
    "from scipy.stats import randint, uniform, loguniform\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from category_encoders import *\n",
    "\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    cross_validate,\n",
    "    RandomizedSearchCV,\n",
    "    StratifiedKFold,\n",
    "    KFold,\n",
    "    cross_val_score\n",
    ")\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler,\n",
    "    OneHotEncoder,\n",
    "    OrdinalEncoder,\n",
    "    FunctionTransformer,\n",
    ")\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.linear_model import LinearRegression, Ridge, ElasticNet\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestRegressor,\n",
    "    GradientBoostingRegressor,\n",
    "    HistGradientBoostingRegressor,\n",
    "    ExtraTreesRegressor\n",
    ")\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    mean_absolute_error,\n",
    "    r2_score,\n",
    ")\n",
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "# ——— Additional models ————————————————————————\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Project root & data paths\n",
    "project_root = Path().resolve().parent\n",
    "df_pre_path = project_root / \"data\" / \"interim\" / \"data_preprocessed.parquet\"\n",
    "df = pd.read_parquet(df_pre_path)\n",
    "\n",
    "src_path = project_root / \"src\"\n",
    "sys.path.append(str(src_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing, I remove outliers in the target variable price, and I log-transform it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Remove outliers in price - although I guess that if I had to put in production, it would be more right to remove outliers only in the training set. \n",
    "\n",
    "Q1 = df[\"price\"].quantile(0.25)\n",
    "Q3 = df[\"price\"].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "df = df[(df[\"price\"] >= lower_bound) & (df[\"price\"] <= upper_bound)]\n",
    "\n",
    "# Target transformation and feature preparation\n",
    "y = np.log1p(df[\"price\"])\n",
    "X = df.drop(columns=[\"price\"])\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I now recall the features in their respective categories in order to be able to feed them to the pipeline, recalling what I did in the previous notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 4. Define feature groups\n",
    "numeric_features = [\n",
    "    'host_listings_count', 'host_total_listings_count',\n",
    "    'accommodates', 'bathrooms', 'bedrooms', 'beds',\n",
    "    'minimum_nights_avg_ntm', 'maximum_nights_avg_ntm',\n",
    "    'number_of_reviews', 'number_of_reviews_ltm', 'number_of_reviews_l30d',\n",
    "    'calculated_host_listings_count',\n",
    "    'calculated_host_listings_count_private_rooms',\n",
    "    'reviews_per_month', 'days_since_host_since',\n",
    "    'air_conditioning', 'elevator', 'fast_wifi', 'parking',\n",
    "    'coffee_machine', 'washer', 'self_check_in', 'streaming_tv',\n",
    "    'dedicated_workspace', 'private_entrance', 'kitchen_appliances',\n",
    "    'heating', 'hot_water', 'safety_equipment', 'clothing_storage',\n",
    "    'balcony', 'premium_views', 'dishwasher', 'gym'\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    'neighbourhood_cleansed',\n",
    "    'property_type',\n",
    "    'room_type',\n",
    "    'host_location'\n",
    "]\n",
    "\n",
    "ordinal_features_days = [\n",
    "    'days_since_first_review',\n",
    "    'days_since_last_review'\n",
    "]\n",
    "\n",
    "# I will keep just one since they are very correlated\n",
    "ordinal_features_reviews = [\n",
    "    'review_scores_rating',\n",
    "    'review_scores_accuracy',\n",
    "    'review_scores_cleanliness',\n",
    "    'review_scores_checkin',\n",
    "    'review_scores_communication',\n",
    "    'review_scores_location',\n",
    "    'review_scores_value']\n",
    "\n",
    "ordinal_features = ordinal_features_days + ordinal_features_reviews\n",
    "\n",
    "# 5. Define category orders for OrdinalEncoder\n",
    "first_review_order = [\n",
    "    'no_review_yet',\n",
    "    'very_new (<= 1 month)',\n",
    "    'new (<= 6 months)',\n",
    "    'established (<= 1 year)',\n",
    "    'mature (<= 3 years)',\n",
    "    'veteran (<= 5 years)',\n",
    "    'legacy (over 5 years)'\n",
    "]\n",
    "\n",
    "last_review_order = [\n",
    "    'no_review',\n",
    "    'very_recent (<= 1 week)',\n",
    "    'recent (<= 1 month)',\n",
    "    'somewhat_recent (<= 3 months)',\n",
    "    'old (<= 6 months)',\n",
    "    'very_old (<= 1 year)',\n",
    "    'dormant (over a year)'\n",
    "]\n",
    "\n",
    "\n",
    "review_order = [\"no_reviews\", \"low_reviews\", \"medium_reviews\", \"high_reviews\", \"top_reviews\"]\n",
    "\n",
    "all_ord_categories = (\n",
    "    [first_review_order, last_review_order] +\n",
    "    [review_order] * len(ordinal_features_reviews)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Build transformers\n",
    "numeric_transformer = Pipeline([\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline([\n",
    "    (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "ordinal_transformer = Pipeline([\n",
    "    (\"ordinal\", OrdinalEncoder(categories=all_ord_categories))\n",
    "])\n",
    "\n",
    "# 7. Combine into ColumnTransformer\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    (\"num\", numeric_transformer,   numeric_features),\n",
    "    (\"cat\", categorical_transformer, categorical_features),\n",
    "    (\"ord\", ordinal_transformer,    ordinal_features),\n",
    "], remainder=\"drop\")\n",
    "\n",
    "# 8. Create full Pipeline\n",
    "model_pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"select\", SelectKBest(k=50)),\n",
    "    (\"regressor\", RandomForestRegressor(random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_and_params = {\n",
    "\n",
    "    \n",
    "    'OLS': {\n",
    "        'model': LinearRegression(),\n",
    "        'params': {}  # No hyperparameters to tune\n",
    "    },\n",
    "    \n",
    "    'Ridge': {\n",
    "        'model': Ridge(random_state=42),\n",
    "        'params': {\n",
    "            'regressor__alpha': loguniform(0.01, 100),\n",
    "            'select__k': randint(20, 100)\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    'ElasticNet': {\n",
    "        'model': ElasticNet(random_state=42),\n",
    "        'params': {\n",
    "            'regressor__alpha': loguniform(0.01, 10),\n",
    "            'regressor__l1_ratio': uniform(0.1, 0.8),\n",
    "            'select__k': randint(20, 100)\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    'RandomForest': {\n",
    "        'model': RandomForestRegressor(n_jobs=-1, random_state=42),\n",
    "        'params': {\n",
    "            'regressor__n_estimators': randint(100, 500),\n",
    "            'regressor__max_depth': randint(5, 20),\n",
    "            'regressor__min_samples_split': randint(2, 10),\n",
    "            'regressor__min_samples_leaf': randint(1, 5),\n",
    "            'select__k': randint(30, 120)\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    'ExtraTrees': {\n",
    "        'model': ExtraTreesRegressor(n_jobs=-1, random_state=42),\n",
    "        'params': {\n",
    "            'regressor__n_estimators': randint(100, 500),\n",
    "            'regressor__max_depth': randint(5, 20),\n",
    "            'regressor__min_samples_split': randint(2, 10),\n",
    "            'regressor__min_samples_leaf': randint(1, 5),\n",
    "            'select__k': randint(30, 120)\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    'HistGB': {\n",
    "        'model': HistGradientBoostingRegressor(random_state=42),\n",
    "        'params': {\n",
    "            'regressor__max_depth': randint(3, 10),\n",
    "            'regressor__learning_rate': loguniform(0.01, 0.3),\n",
    "            'regressor__max_iter': randint(100, 500),\n",
    "            'regressor__l2_regularization': loguniform(1e-4, 10),\n",
    "            'select__k': randint(30, 120)\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    'GradientBoosting': {\n",
    "        'model': GradientBoostingRegressor(random_state=42),\n",
    "        'params': {\n",
    "            'regressor__n_estimators': randint(100, 500),\n",
    "            'regressor__learning_rate': loguniform(0.01, 0.3),\n",
    "            'regressor__max_depth': randint(3, 10),\n",
    "            'regressor__subsample': uniform(0.6, 0.4),\n",
    "            'select__k': randint(30, 120)\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    'KNN': {\n",
    "        'model': KNeighborsRegressor(),\n",
    "        'params': {\n",
    "            'regressor__n_neighbors': randint(3, 20),\n",
    "            'regressor__weights': ['uniform', 'distance'],\n",
    "            'regressor__p': [1, 2],\n",
    "            'select__k': randint(20, 80)\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    'SVR': {\n",
    "        'model': SVR(),\n",
    "        'params': {\n",
    "            'regressor__C': loguniform(0.1, 100),\n",
    "            'regressor__epsilon': loguniform(0.01, 1),\n",
    "            'regressor__kernel': ['rbf', 'linear'],\n",
    "            'select__k': randint(20, 80)\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    \n",
    "    'XGBoost': {\n",
    "        'model': XGBRegressor(objective=\"reg:squarederror\", n_jobs=-1, random_state=42),\n",
    "        'params': {\n",
    "            'regressor__n_estimators': randint(200, 1000),\n",
    "            'regressor__max_depth': randint(4, 12),\n",
    "            'regressor__learning_rate': loguniform(0.01, 0.3),\n",
    "            'regressor__subsample': uniform(0.6, 0.4),\n",
    "            'regressor__colsample_bytree': uniform(0.6, 0.4),\n",
    "            'regressor__gamma': loguniform(1e-8, 1e-1),\n",
    "            'select__k': randint(30, 120)\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    'CatBoost': {\n",
    "        'model': CatBoostRegressor(\n",
    "            loss_function=\"RMSE\",\n",
    "            random_seed=42,\n",
    "            verbose=0,\n",
    "            allow_writing_files=False\n",
    "        ),\n",
    "        'params': {\n",
    "            'regressor__iterations': randint(200, 1000),\n",
    "            'regressor__depth': randint(4, 10),\n",
    "            'regressor__learning_rate': loguniform(0.01, 0.3),\n",
    "            'regressor__l2_leaf_reg': loguniform(1, 10),\n",
    "            'select__k': randint(30, 120)\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I will run a Nested Cross Validation since I want to compare different models with different parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_cross_validation_regression(X, y, models_and_params, outer_cv=5, inner_cv=3, \n",
    "                                     n_iter=20, scoring='r2', random_state=42):\n",
    "    \"\"\"\n",
    "    Perform nested cross-validation for regression model selection and performance estimation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Feature matrix\n",
    "    y : array-like, shape (n_samples,)\n",
    "        Target vector\n",
    "    models_and_params : dict\n",
    "        Dictionary containing models and their hyperparameter grids\n",
    "    outer_cv : int\n",
    "        Number of folds for outer cross-validation\n",
    "    inner_cv : int\n",
    "        Number of folds for inner cross-validation (hyperparameter tuning)\n",
    "    n_iter : int\n",
    "        Number of parameter settings sampled for RandomizedSearchCV\n",
    "    scoring : str\n",
    "        Scoring metric\n",
    "    random_state : int\n",
    "        Random state for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    results : dict\n",
    "        Dictionary containing results for each model\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize cross-validation splitters\n",
    "    outer_cv_splitter = KFold(n_splits=outer_cv, shuffle=True, random_state=random_state)\n",
    "    inner_cv_splitter = KFold(n_splits=inner_cv, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    print(\"=== NESTED CROSS-VALIDATION FOR AIRBNB PRICE PREDICTION ===\\n\")\n",
    "    print(f\"Dataset shape: {X.shape}\")\n",
    "    print(f\"Target range: {y.min():.3f} - {y.max():.3f} (log-transformed)\")\n",
    "    print(f\"Outer CV: {outer_cv} folds, Inner CV: {inner_cv} folds\")\n",
    "    print(f\"Hyperparameter search iterations: {n_iter}\\n\")\n",
    "    \n",
    "    for model_name, model_config in models_and_params.items():\n",
    "        print(f\"Evaluating {model_name}...\")\n",
    "        \n",
    "        # Create pipeline with preprocessing, feature selection, and classifier\n",
    "        pipeline = Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('select', SelectKBest(k=50)),  # Default value, will be tuned if in params\n",
    "            ('regressor', model_config['model'])\n",
    "        ])\n",
    "        \n",
    "        # Outer loop: Performance estimation\n",
    "        outer_scores = []\n",
    "        best_params_per_fold = []\n",
    "        \n",
    "        fold = 1\n",
    "        for train_idx, test_idx in outer_cv_splitter.split(X):\n",
    "            print(f\"  Processing outer fold {fold}/{outer_cv}\")\n",
    "            \n",
    "            X_train_outer, X_test_outer = X.iloc[train_idx], X.iloc[test_idx]\n",
    "            y_train_outer, y_test_outer = y.iloc[train_idx], y.iloc[test_idx]\n",
    "            \n",
    "            # Inner loop: Hyperparameter optimization\n",
    "            if model_config['params']:  # Only tune if there are parameters to tune\n",
    "                search = RandomizedSearchCV(\n",
    "                    pipeline, \n",
    "                    model_config['params'],\n",
    "                    cv=inner_cv_splitter,\n",
    "                    scoring=scoring,\n",
    "                    n_iter=n_iter,\n",
    "                    n_jobs=-1,\n",
    "                    random_state=random_state,\n",
    "                    return_train_score=False\n",
    "                )\n",
    "                \n",
    "                # Fit grid search on outer training set\n",
    "                search.fit(X_train_outer, y_train_outer)\n",
    "                \n",
    "                # Get best model from inner CV\n",
    "                best_model = search.best_estimator_\n",
    "                best_params_per_fold.append(search.best_params_)\n",
    "            else:\n",
    "                # No hyperparameters to tune, just fit the pipeline\n",
    "                pipeline.fit(X_train_outer, y_train_outer)\n",
    "                best_model = pipeline\n",
    "                best_params_per_fold.append({})\n",
    "            \n",
    "            # Evaluate best model on outer test set\n",
    "            y_pred = best_model.predict(X_test_outer)\n",
    "            if scoring == 'r2':\n",
    "                fold_score = r2_score(y_test_outer, y_pred)\n",
    "            elif scoring == 'neg_mean_squared_error':\n",
    "                fold_score = -mean_squared_error(y_test_outer, y_pred)\n",
    "            elif scoring == 'neg_mean_absolute_error':\n",
    "                fold_score = -mean_absolute_error(y_test_outer, y_pred)\n",
    "            else:\n",
    "                fold_score = best_model.score(X_test_outer, y_test_outer)\n",
    "            \n",
    "            outer_scores.append(fold_score)\n",
    "            fold += 1\n",
    "        \n",
    "        # Store results\n",
    "        results[model_name] = {\n",
    "            'outer_scores': outer_scores,\n",
    "            'mean_score': np.mean(outer_scores),\n",
    "            'std_score': np.std(outer_scores),\n",
    "            'best_params_per_fold': best_params_per_fold\n",
    "        }\n",
    "        \n",
    "        print(f\"  Mean {scoring}: {np.mean(outer_scores):.4f} (+/- {np.std(outer_scores):.4f})\")\n",
    "        print(f\"  Individual fold scores: {[f'{score:.4f}' for score in outer_scores]}\")\n",
    "        print()\n",
    "    \n",
    "    return results\n",
    "\n",
    "def select_best_model_and_retrain(X, y, models_and_params, results, inner_cv=3, n_iter=50):\n",
    "    \"\"\"\n",
    "    Select the best model based on nested CV results and retrain on full dataset.\n",
    "    \"\"\"\n",
    "    # Find best model\n",
    "    best_model_name = max(results.keys(), key=lambda k: results[k]['mean_score'])\n",
    "    best_model_config = models_and_params[best_model_name]\n",
    "    \n",
    "    print(f\"=== BEST MODEL SELECTION ===\")\n",
    "    print(f\"Best model: {best_model_name}\")\n",
    "    print(f\"Expected performance: {results[best_model_name]['mean_score']:.4f} \"\n",
    "          f\"(+/- {results[best_model_name]['std_score']:.4f})\")\n",
    "    print()\n",
    "    \n",
    "    # Retrain best model on full dataset with hyperparameter tuning\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('select', SelectKBest(k=50)),\n",
    "        ('regressor', best_model_config['model'])\n",
    "    ])\n",
    "    \n",
    "    if best_model_config['params']:\n",
    "        inner_cv_splitter = KFold(n_splits=inner_cv, shuffle=True, random_state=42)\n",
    "        \n",
    "        final_search = RandomizedSearchCV(\n",
    "            pipeline,\n",
    "            best_model_config['params'],\n",
    "            cv=inner_cv_splitter,\n",
    "            scoring='r2',\n",
    "            n_iter=n_iter,\n",
    "            n_jobs=-1,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        final_search.fit(X, y)\n",
    "        final_model = final_search.best_estimator_\n",
    "        \n",
    "        print(f\"Final model hyperparameters: {final_search.best_params_}\")\n",
    "        print(f\"Cross-validation score on full dataset: {final_search.best_score_:.4f}\")\n",
    "    else:\n",
    "        pipeline.fit(X, y)\n",
    "        final_model = pipeline\n",
    "        print(\"No hyperparameters to tune for this model.\")\n",
    "    \n",
    "    return final_model, best_model_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Nested Cross-Validation for Airbnb Price Prediction...\n",
      "Training set shape: (16176, 53)\n",
      "Test set shape: (4045, 53)\n",
      "\n",
      "=== NESTED CROSS-VALIDATION FOR AIRBNB PRICE PREDICTION ===\n",
      "\n",
      "Dataset shape: (16176, 53)\n",
      "Target range: 2.708 - 5.897 (log-transformed)\n",
      "Outer CV: 5 folds, Inner CV: 3 folds\n",
      "Hyperparameter search iterations: 20\n",
      "\n",
      "Evaluating OLS...\n",
      "  Processing outer fold 1/5\n",
      "  Processing outer fold 2/5\n",
      "  Processing outer fold 3/5\n",
      "  Processing outer fold 4/5\n",
      "  Processing outer fold 5/5\n",
      "  Mean r2: 0.3862 (+/- 0.0158)\n",
      "  Individual fold scores: ['0.3759', '0.3875', '0.3971', '0.3627', '0.4078']\n",
      "\n",
      "Evaluating Ridge...\n",
      "  Processing outer fold 1/5\n",
      "  Processing outer fold 2/5\n",
      "  Processing outer fold 3/5\n",
      "  Processing outer fold 4/5\n",
      "  Processing outer fold 5/5\n",
      "  Mean r2: 0.4163 (+/- 0.0191)\n",
      "  Individual fold scores: ['0.4011', '0.4112', '0.4302', '0.3935', '0.4455']\n",
      "\n",
      "Evaluating ElasticNet...\n",
      "  Processing outer fold 1/5\n",
      "  Processing outer fold 2/5\n",
      "  Processing outer fold 3/5\n",
      "  Processing outer fold 4/5\n",
      "  Processing outer fold 5/5\n",
      "  Mean r2: 0.3655 (+/- 0.0180)\n",
      "  Individual fold scores: ['0.3586', '0.3588', '0.3778', '0.3399', '0.3925']\n",
      "\n",
      "Evaluating RandomForest...\n",
      "  Processing outer fold 1/5\n",
      "  Processing outer fold 2/5\n",
      "  Processing outer fold 3/5\n",
      "  Processing outer fold 4/5\n",
      "  Processing outer fold 5/5\n",
      "  Mean r2: 0.5002 (+/- 0.0175)\n",
      "  Individual fold scores: ['0.4918', '0.5000', '0.4917', '0.4840', '0.5337']\n",
      "\n",
      "Evaluating ExtraTrees...\n",
      "  Processing outer fold 1/5\n",
      "  Processing outer fold 2/5\n",
      "  Processing outer fold 3/5\n",
      "  Processing outer fold 4/5\n",
      "  Processing outer fold 5/5\n",
      "  Mean r2: 0.4911 (+/- 0.0164)\n",
      "  Individual fold scores: ['0.4826', '0.4852', '0.4864', '0.4779', '0.5235']\n",
      "\n",
      "Evaluating HistGB...\n",
      "  Processing outer fold 1/5\n",
      "  Processing outer fold 2/5\n",
      "  Processing outer fold 3/5\n",
      "  Processing outer fold 4/5\n",
      "  Processing outer fold 5/5\n",
      "  Mean r2: 0.5422 (+/- 0.0158)\n",
      "  Individual fold scores: ['0.5367', '0.5423', '0.5343', '0.5258', '0.5721']\n",
      "\n",
      "Evaluating GradientBoosting...\n",
      "  Processing outer fold 1/5\n",
      "  Processing outer fold 2/5\n",
      "  Processing outer fold 3/5\n",
      "  Processing outer fold 4/5\n",
      "  Processing outer fold 5/5\n",
      "  Mean r2: 0.5444 (+/- 0.0161)\n",
      "  Individual fold scores: ['0.5308', '0.5453', '0.5341', '0.5369', '0.5750']\n",
      "\n",
      "Evaluating KNN...\n",
      "  Processing outer fold 1/5\n",
      "  Processing outer fold 2/5\n",
      "  Processing outer fold 3/5\n",
      "  Processing outer fold 4/5\n",
      "  Processing outer fold 5/5\n",
      "  Mean r2: 0.3621 (+/- 0.0073)\n",
      "  Individual fold scores: ['0.3618', '0.3601', '0.3722', '0.3501', '0.3664']\n",
      "\n",
      "Evaluating SVR...\n",
      "  Processing outer fold 1/5\n",
      "  Processing outer fold 2/5\n",
      "  Processing outer fold 3/5\n",
      "  Processing outer fold 4/5\n",
      "  Processing outer fold 5/5\n",
      "  Mean r2: 0.4589 (+/- 0.0139)\n",
      "  Individual fold scores: ['0.4510', '0.4525', '0.4578', '0.4473', '0.4859']\n",
      "\n",
      "Evaluating XGBoost...\n",
      "  Processing outer fold 1/5\n",
      "  Processing outer fold 2/5\n",
      "  Processing outer fold 3/5\n",
      "  Processing outer fold 4/5\n",
      "  Processing outer fold 5/5\n",
      "  Mean r2: 0.5639 (+/- 0.0151)\n",
      "  Individual fold scores: ['0.5557', '0.5617', '0.5547', '0.5541', '0.5936']\n",
      "\n",
      "Evaluating CatBoost...\n",
      "  Processing outer fold 1/5\n",
      "  Processing outer fold 2/5\n",
      "  Processing outer fold 3/5\n",
      "  Processing outer fold 4/5\n",
      "  Processing outer fold 5/5\n",
      "  Mean r2: 0.5567 (+/- 0.0162)\n",
      "  Individual fold scores: ['0.5485', '0.5543', '0.5487', '0.5437', '0.5883']\n",
      "\n",
      "=== BEST MODEL SELECTION ===\n",
      "Best model: XGBoost\n",
      "Expected performance: 0.5639 (+/- 0.0151)\n",
      "\n",
      "Final model hyperparameters: {'regressor__colsample_bytree': np.float64(0.8663689426469987), 'regressor__gamma': np.float64(0.00013774775007214802), 'regressor__learning_rate': np.float64(0.02545642421255076), 'regressor__max_depth': 6, 'regressor__n_estimators': 938, 'regressor__subsample': np.float64(0.6478376983753207), 'select__k': 107}\n",
      "Cross-validation score on full dataset: 0.5600\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Starting Nested Cross-Validation for Airbnb Price Prediction...\")\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "print()\n",
    "\n",
    "# Perform nested cross-validation (reduced subset for demonstration)\n",
    "# You can include all models by using the full models_and_params dictionary\n",
    "selected_models = models_and_params\n",
    "\n",
    "\n",
    "cv_results = nested_cross_validation_regression(\n",
    "    X_train, y_train, \n",
    "    selected_models,  # Use selected_models or models_and_params for all\n",
    "    outer_cv=5, \n",
    "    inner_cv=3, \n",
    "    n_iter=20,  # Reduced for faster execution\n",
    "    scoring='r2'\n",
    ")\n",
    "\n",
    "# Select and retrain best model\n",
    "final_model, best_model_name = select_best_model_and_retrain(\n",
    "    X_train, y_train, selected_models, cv_results, n_iter=30\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FINAL EVALUATION ON TEST SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== FINAL TEST SET EVALUATION ===\n",
      "Test R²: 0.5870\n",
      "Test RMSE: 0.3230\n",
      "Test MAE: 0.2468\n",
      "\n",
      "In original price scale:\n",
      "Test RMSE: $49.26\n",
      "Test MAE: $34.51\n",
      "\n",
      "=== NESTED CV SUMMARY ===\n",
      "               Model  Mean_R2  Std_R2  CI_Lower  CI_Upper\n",
      "9            XGBoost   0.5639  0.0151    0.5344    0.5935\n",
      "10          CatBoost   0.5567  0.0162    0.5250    0.5884\n",
      "6   GradientBoosting   0.5444  0.0161    0.5129    0.5759\n",
      "5             HistGB   0.5422  0.0158    0.5112    0.5733\n",
      "3       RandomForest   0.5002  0.0175    0.4660    0.5345\n",
      "4         ExtraTrees   0.4911  0.0164    0.4589    0.5234\n",
      "8                SVR   0.4589  0.0139    0.4316    0.4862\n",
      "1              Ridge   0.4163  0.0191    0.3789    0.4537\n",
      "0                OLS   0.3862  0.0158    0.3553    0.4171\n",
      "2         ElasticNet   0.3655  0.0180    0.3302    0.4009\n",
      "7                KNN   0.3621  0.0073    0.3478    0.3765\n",
      "\n",
      "Selected model: XGBoost\n",
      "Note: The nested CV performance estimates represent unbiased estimates\n",
      "of how well each model is expected to perform on unseen data.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"\\n=== FINAL TEST SET EVALUATION ===\")\n",
    "y_pred_test = final_model.predict(X_test)\n",
    "\n",
    "test_r2 = r2_score(y_test, y_pred_test)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "\n",
    "print(f\"Test R²: {test_r2:.4f}\")\n",
    "print(f\"Test RMSE: {test_rmse:.4f}\")\n",
    "print(f\"Test MAE: {test_mae:.4f}\")\n",
    "\n",
    "# Convert back from log scale for interpretation\n",
    "y_test_original = np.expm1(y_test)\n",
    "y_pred_original = np.expm1(y_pred_test)\n",
    "\n",
    "test_rmse_original = np.sqrt(mean_squared_error(y_test_original, y_pred_original))\n",
    "test_mae_original = mean_absolute_error(y_test_original, y_pred_original)\n",
    "\n",
    "print(f\"\\nIn original price scale:\")\n",
    "print(f\"Test RMSE: ${test_rmse_original:.2f}\")\n",
    "print(f\"Test MAE: ${test_mae_original:.2f}\")\n",
    "\n",
    "# Summary of all models\n",
    "print(\"\\n=== NESTED CV SUMMARY ===\")\n",
    "cv_summary = pd.DataFrame([\n",
    "    {\n",
    "        'Model': model_name,\n",
    "        'Mean_R2': result['mean_score'],\n",
    "        'Std_R2': result['std_score'],\n",
    "        'CI_Lower': result['mean_score'] - 1.96 * result['std_score'],\n",
    "        'CI_Upper': result['mean_score'] + 1.96 * result['std_score']\n",
    "    }\n",
    "    for model_name, result in cv_results.items()\n",
    "]).sort_values('Mean_R2', ascending=False)\n",
    "\n",
    "print(cv_summary.round(4))\n",
    "\n",
    "print(f\"\\nSelected model: {best_model_name}\")\n",
    "print(\"Note: The nested CV performance estimates represent unbiased estimates\")\n",
    "print(\"of how well each model is expected to perform on unseen data.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
