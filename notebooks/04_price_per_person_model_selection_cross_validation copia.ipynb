{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **04 MODELING (NESTED CROSS-VALIDATION)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns  \n",
    "\n",
    "from scipy.stats import randint, uniform, loguniform\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from category_encoders import *\n",
    "\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    cross_validate,\n",
    "    RandomizedSearchCV,\n",
    "    StratifiedKFold,\n",
    "    KFold,\n",
    "    cross_val_score\n",
    ")\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler,\n",
    "    OneHotEncoder,\n",
    "    OrdinalEncoder,\n",
    "    FunctionTransformer,\n",
    ")\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.linear_model import LinearRegression, Ridge, ElasticNet\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestRegressor,\n",
    "    GradientBoostingRegressor,\n",
    "    HistGradientBoostingRegressor,\n",
    "    ExtraTreesRegressor\n",
    ")\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    mean_absolute_error,\n",
    "    r2_score,\n",
    ")\n",
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "# ——— Additional models ————————————————————————\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Project root & data paths\n",
    "project_root = Path().resolve().parent\n",
    "df_pre_path = project_root / \"data\" / \"interim\" / \"data_preprocessed.parquet\"\n",
    "df = pd.read_parquet(df_pre_path)\n",
    "\n",
    "src_path = project_root / \"src\"\n",
    "sys.path.append(str(src_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before removing outlier from the training set only, to avoid data leakage, I divide into training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# 1. Split FIRST on raw data\\nX = df.drop(columns=[\"price\"])\\n\\ndf[\"price_per_person\"] = df[\"price\"] / df[\"accommodates\"]\\ny = df[\"price_per_person\"]\\n\\nX_train, X_test, y_train, y_test = train_test_split(\\n    X, y, test_size=0.2, random_state=42\\n)'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# 1. Split FIRST on raw data\n",
    "X = df.drop(columns=[\"price\"])\n",
    "\n",
    "df[\"price_per_person\"] = df[\"price\"] / df[\"accommodates\"]\n",
    "y = df[\"price_per_person\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, I remove outliers in the target variable price, only on the training set, and I log-transform it.\n",
    "\n",
    "todo - add price per accomodate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\\nQ1 = y_train.quantile(0.25)  # Only training data\\nQ3 = y_train.quantile(0.75)  # Only training data\\nIQR = Q3 - Q1\\n\\nlower_bound = Q1 - 1.5 * IQR\\nupper_bound = Q3 + 1.5 * IQR\\n\\n# Apply to training set only\\ntrain_mask = (y_train >= lower_bound) & (y_train <= upper_bound)\\nX_train_clean = X_train[train_mask]\\ny_train_clean = y_train[train_mask]\\n\\n# 3. Log-transform after outlier removal\\ny_train_log = np.log1p(y_train_clean)\\ny_test_log = np.log1p(y_test)  # Transform test, but don\\'t remove outliers\\n\\n# 4. Continue with nested CV using X_train_clean, y_train_log'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\"\n",
    "Q1 = y_train.quantile(0.25)  # Only training data\n",
    "Q3 = y_train.quantile(0.75)  # Only training data\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Apply to training set only\n",
    "train_mask = (y_train >= lower_bound) & (y_train <= upper_bound)\n",
    "X_train_clean = X_train[train_mask]\n",
    "y_train_clean = y_train[train_mask]\n",
    "\n",
    "# 3. Log-transform after outlier removal\n",
    "y_train_log = np.log1p(y_train_clean)\n",
    "y_test_log = np.log1p(y_test)  # Transform test, but don't remove outliers\n",
    "\n",
    "# 4. Continue with nested CV using X_train_clean, y_train_log\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price per person statistics:\n",
      "count    21961.000000\n",
      "mean        73.041143\n",
      "std        432.687976\n",
      "min          3.000000\n",
      "25%         31.250000\n",
      "50%         45.000000\n",
      "75%         68.500000\n",
      "max      44444.000000\n",
      "Name: price_per_person, dtype: float64\n",
      "Values above €1000 per person: 77\n",
      "Values above €500 per person: 117\n",
      "Removed 154 extreme outliers\n"
     ]
    }
   ],
   "source": [
    "# 1. First, examine your price_per_person distribution BEFORE any processing\n",
    "df[\"price_per_person\"] = df[\"price\"] / df[\"accommodates\"]\n",
    "\n",
    "# Check for extreme values\n",
    "print(\"Price per person statistics:\")\n",
    "print(df[\"price_per_person\"].describe())\n",
    "print(f\"Values above €1000 per person: {(df['price_per_person'] > 1000).sum()}\")\n",
    "print(f\"Values above €500 per person: {(df['price_per_person'] > 500).sum()}\")\n",
    "\n",
    "# 2. Create a reasonable filter for price per person\n",
    "# Most Airbnb prices per person should be between €10-300 per night\n",
    "reasonable_mask = (df[\"price_per_person\"] >= 10) & (df[\"price_per_person\"] <= 500)\n",
    "df_filtered = df[reasonable_mask].copy()\n",
    "\n",
    "print(f\"Removed {len(df) - len(df_filtered)} extreme outliers\")\n",
    "\n",
    "# 3. Now proceed with your train/test split on the filtered data\n",
    "X = df_filtered.drop(columns=[\"price\", \"price_per_person\"])\n",
    "y = df_filtered[\"price_per_person\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 4. Apply IQR outlier removal to training set only\n",
    "Q1 = y_train.quantile(0.25)\n",
    "Q3 = y_train.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "train_mask = (y_train >= lower_bound) & (y_train <= upper_bound)\n",
    "X_train_clean = X_train[train_mask]\n",
    "y_train_clean = y_train[train_mask]\n",
    "\n",
    "# 5. Log transform\n",
    "y_train_log = np.log1p(y_train_clean)\n",
    "y_test_log = np.log1p(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I now recall the features in their respective categories in order to be able to feed them to the pipeline, recalling what I did in the previous notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 4. Define feature groups\n",
    "numeric_features = [\n",
    "    'host_listings_count', 'host_total_listings_count', 'bathrooms', 'bedrooms', 'beds',\n",
    "    'minimum_nights_avg_ntm', 'maximum_nights_avg_ntm',\n",
    "    'number_of_reviews', 'number_of_reviews_ltm', 'number_of_reviews_l30d',\n",
    "    'calculated_host_listings_count',\n",
    "    'calculated_host_listings_count_private_rooms',\n",
    "    'reviews_per_month', 'days_since_host_since',\n",
    "    'air_conditioning', 'elevator', 'fast_wifi', 'parking',\n",
    "    'coffee_machine', 'washer', 'self_check_in', 'streaming_tv',\n",
    "    'dedicated_workspace', 'private_entrance', 'kitchen_appliances',\n",
    "    'heating', 'hot_water', 'safety_equipment', 'clothing_storage',\n",
    "    'balcony', 'premium_views', 'dishwasher', 'gym'\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    'neighbourhood_cleansed',\n",
    "    'property_type',\n",
    "    'room_type',\n",
    "    'host_location'\n",
    "]\n",
    "\n",
    "ordinal_features_days = [\n",
    "    'days_since_first_review',\n",
    "    'days_since_last_review'\n",
    "]\n",
    "\n",
    "# I will keep just one since they are very correlated\n",
    "ordinal_features_reviews = [\n",
    "    'review_scores_rating',\n",
    "    'review_scores_accuracy',\n",
    "    'review_scores_cleanliness',\n",
    "    'review_scores_checkin',\n",
    "    'review_scores_communication',\n",
    "    'review_scores_location',\n",
    "    'review_scores_value']\n",
    "\n",
    "ordinal_features = ordinal_features_days + ordinal_features_reviews\n",
    "\n",
    "# 5. Define category orders for OrdinalEncoder\n",
    "first_review_order = [\n",
    "    'no_review_yet',\n",
    "    'very_new (<= 1 month)',\n",
    "    'new (<= 6 months)',\n",
    "    'established (<= 1 year)',\n",
    "    'mature (<= 3 years)',\n",
    "    'veteran (<= 5 years)',\n",
    "    'legacy (over 5 years)'\n",
    "]\n",
    "\n",
    "last_review_order = [\n",
    "    'no_review',\n",
    "    'very_recent (<= 1 week)',\n",
    "    'recent (<= 1 month)',\n",
    "    'somewhat_recent (<= 3 months)',\n",
    "    'old (<= 6 months)',\n",
    "    'very_old (<= 1 year)',\n",
    "    'dormant (over a year)'\n",
    "]\n",
    "\n",
    "\n",
    "review_order = [\"no_reviews\", \"low_reviews\", \"medium_reviews\", \"high_reviews\", \"top_reviews\"]\n",
    "\n",
    "all_ord_categories = (\n",
    "    [first_review_order, last_review_order] +\n",
    "    [review_order] * len(ordinal_features_reviews)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Build transformers\n",
    "numeric_transformer = Pipeline([\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline([\n",
    "    (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "ordinal_transformer = Pipeline([\n",
    "    (\"ordinal\", OrdinalEncoder(categories=all_ord_categories))\n",
    "])\n",
    "\n",
    "# 7. Combine into ColumnTransformer\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    (\"num\", numeric_transformer,   numeric_features),\n",
    "    (\"cat\", categorical_transformer, categorical_features),\n",
    "    (\"ord\", ordinal_transformer,    ordinal_features),\n",
    "], remainder=\"drop\")\n",
    "\n",
    "# 8. Create full Pipeline\n",
    "model_pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"select\", SelectKBest(k=50)),\n",
    "    (\"regressor\", RandomForestRegressor(random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_and_params = {\n",
    "\n",
    "    \n",
    "    'OLS': {\n",
    "        'model': LinearRegression(),\n",
    "        'params': {}  # No hyperparameters to tune\n",
    "    },\n",
    "    \n",
    "    'Ridge': {\n",
    "        'model': Ridge(random_state=42),\n",
    "        'params': {\n",
    "            'regressor__alpha': loguniform(0.01, 100),\n",
    "            'select__k': randint(20, 100)\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    'ElasticNet': {\n",
    "        'model': ElasticNet(random_state=42),\n",
    "        'params': {\n",
    "            'regressor__alpha': loguniform(0.01, 10),\n",
    "            'regressor__l1_ratio': uniform(0.1, 0.8),\n",
    "            'select__k': randint(20, 100)\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    'RandomForest': {\n",
    "        'model': RandomForestRegressor(n_jobs=-1, random_state=42),\n",
    "        'params': {\n",
    "            'regressor__n_estimators': randint(100, 500),\n",
    "            'regressor__max_depth': randint(5, 20),\n",
    "            'regressor__min_samples_split': randint(2, 10),\n",
    "            'regressor__min_samples_leaf': randint(1, 5),\n",
    "            'select__k': randint(30, 120)\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    'ExtraTrees': {\n",
    "        'model': ExtraTreesRegressor(n_jobs=-1, random_state=42),\n",
    "        'params': {\n",
    "            'regressor__n_estimators': randint(100, 500),\n",
    "            'regressor__max_depth': randint(5, 20),\n",
    "            'regressor__min_samples_split': randint(2, 10),\n",
    "            'regressor__min_samples_leaf': randint(1, 5),\n",
    "            'select__k': randint(30, 120)\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    'HistGB': {\n",
    "        'model': HistGradientBoostingRegressor(random_state=42),\n",
    "        'params': {\n",
    "            'regressor__max_depth': randint(3, 10),\n",
    "            'regressor__learning_rate': loguniform(0.01, 0.3),\n",
    "            'regressor__max_iter': randint(100, 500),\n",
    "            'regressor__l2_regularization': loguniform(1e-4, 10),\n",
    "            'select__k': randint(30, 120)\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    'GradientBoosting': {\n",
    "        'model': GradientBoostingRegressor(random_state=42),\n",
    "        'params': {\n",
    "            'regressor__n_estimators': randint(100, 500),\n",
    "            'regressor__learning_rate': loguniform(0.01, 0.3),\n",
    "            'regressor__max_depth': randint(3, 10),\n",
    "            'regressor__subsample': uniform(0.6, 0.4),\n",
    "            'select__k': randint(30, 120)\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    'KNN': {\n",
    "        'model': KNeighborsRegressor(),\n",
    "        'params': {\n",
    "            'regressor__n_neighbors': randint(3, 20),\n",
    "            'regressor__weights': ['uniform', 'distance'],\n",
    "            'regressor__p': [1, 2],\n",
    "            'select__k': randint(20, 80)\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    'SVR': {\n",
    "        'model': SVR(),\n",
    "        'params': {\n",
    "            'regressor__C': loguniform(0.1, 100),\n",
    "            'regressor__epsilon': loguniform(0.01, 1),\n",
    "            'regressor__kernel': ['rbf', 'linear'],\n",
    "            'select__k': randint(20, 80)\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    \n",
    "    'XGBoost': {\n",
    "        'model': XGBRegressor(objective=\"reg:squarederror\", n_jobs=-1, random_state=42),\n",
    "        'params': {\n",
    "            'regressor__n_estimators': randint(200, 1000),\n",
    "            'regressor__max_depth': randint(4, 12),\n",
    "            'regressor__learning_rate': loguniform(0.01, 0.3),\n",
    "            'regressor__subsample': uniform(0.6, 0.4),\n",
    "            'regressor__colsample_bytree': uniform(0.6, 0.4),\n",
    "            'regressor__gamma': loguniform(1e-8, 1e-1),\n",
    "            'select__k': randint(30, 120)\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    'CatBoost': {\n",
    "        'model': CatBoostRegressor(\n",
    "            loss_function=\"RMSE\",\n",
    "            random_seed=42,\n",
    "            verbose=0,\n",
    "            allow_writing_files=False\n",
    "        ),\n",
    "        'params': {\n",
    "            'regressor__iterations': randint(200, 1000),\n",
    "            'regressor__depth': randint(4, 10),\n",
    "            'regressor__learning_rate': loguniform(0.01, 0.3),\n",
    "            'regressor__l2_leaf_reg': loguniform(1, 10),\n",
    "            'select__k': randint(30, 120)\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I will run a Nested Cross Validation since I want to compare different models with different parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_cross_validation_regression(X, y, models_and_params, outer_cv=5, inner_cv=3, \n",
    "                                     n_iter=20, scoring='r2', random_state=42):\n",
    "    \"\"\"\n",
    "    Perform nested cross-validation for regression model selection and performance estimation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Feature matrix\n",
    "    y : array-like, shape (n_samples,)\n",
    "        Target vector\n",
    "    models_and_params : dict\n",
    "        Dictionary containing models and their hyperparameter grids\n",
    "    outer_cv : int\n",
    "        Number of folds for outer cross-validation\n",
    "    inner_cv : int\n",
    "        Number of folds for inner cross-validation (hyperparameter tuning)\n",
    "    n_iter : int\n",
    "        Number of parameter settings sampled for RandomizedSearchCV\n",
    "    scoring : str\n",
    "        Scoring metric\n",
    "    random_state : int\n",
    "        Random state for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    results : dict\n",
    "        Dictionary containing results for each model\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize cross-validation splitters\n",
    "    outer_cv_splitter = KFold(n_splits=outer_cv, shuffle=True, random_state=random_state)\n",
    "    inner_cv_splitter = KFold(n_splits=inner_cv, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    print(\"=== NESTED CROSS-VALIDATION FOR AIRBNB PRICE PREDICTION ===\\n\")\n",
    "    print(f\"Dataset shape: {X.shape}\")\n",
    "    print(f\"Target range: {y.min():.3f} - {y.max():.3f} (log-transformed)\")\n",
    "    print(f\"Outer CV: {outer_cv} folds, Inner CV: {inner_cv} folds\")\n",
    "    print(f\"Hyperparameter search iterations: {n_iter}\\n\")\n",
    "    \n",
    "    for model_name, model_config in models_and_params.items():\n",
    "        print(f\"Evaluating {model_name}...\")\n",
    "        \n",
    "        # Create pipeline with preprocessing, feature selection, and classifier\n",
    "        pipeline = Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('select', SelectKBest(k=50)),  # Default value, will be tuned if in params\n",
    "            ('regressor', model_config['model'])\n",
    "        ])\n",
    "        \n",
    "        # Outer loop: Performance estimation\n",
    "        outer_scores = []\n",
    "        best_params_per_fold = []\n",
    "        \n",
    "        fold = 1\n",
    "        for train_idx, test_idx in outer_cv_splitter.split(X):\n",
    "            print(f\"  Processing outer fold {fold}/{outer_cv}\")\n",
    "            \n",
    "            X_train_outer, X_test_outer = X.iloc[train_idx], X.iloc[test_idx]\n",
    "            y_train_outer, y_test_outer = y.iloc[train_idx], y.iloc[test_idx]\n",
    "            \n",
    "            # Inner loop: Hyperparameter optimization\n",
    "            if model_config['params']:  # Only tune if there are parameters to tune\n",
    "                search = RandomizedSearchCV(\n",
    "                    pipeline, \n",
    "                    model_config['params'],\n",
    "                    cv=inner_cv_splitter,\n",
    "                    scoring=scoring,\n",
    "                    n_iter=n_iter,\n",
    "                    n_jobs=-1,\n",
    "                    random_state=random_state,\n",
    "                    return_train_score=False\n",
    "                )\n",
    "                \n",
    "                # Fit grid search on outer training set\n",
    "                search.fit(X_train_outer, y_train_outer)\n",
    "                \n",
    "                # Get best model from inner CV\n",
    "                best_model = search.best_estimator_\n",
    "                best_params_per_fold.append(search.best_params_)\n",
    "            else:\n",
    "                # No hyperparameters to tune, just fit the pipeline\n",
    "                pipeline.fit(X_train_outer, y_train_outer)\n",
    "                best_model = pipeline\n",
    "                best_params_per_fold.append({})\n",
    "            \n",
    "            # Evaluate best model on outer test set\n",
    "            y_pred = best_model.predict(X_test_outer)\n",
    "            if scoring == 'r2':\n",
    "                fold_score = r2_score(y_test_outer, y_pred)\n",
    "            elif scoring == 'neg_mean_squared_error':\n",
    "                fold_score = -mean_squared_error(y_test_outer, y_pred)\n",
    "            elif scoring == 'neg_mean_absolute_error':\n",
    "                fold_score = -mean_absolute_error(y_test_outer, y_pred)\n",
    "            else:\n",
    "                fold_score = best_model.score(X_test_outer, y_test_outer)\n",
    "            \n",
    "            outer_scores.append(fold_score)\n",
    "            fold += 1\n",
    "        \n",
    "        # Store results\n",
    "        results[model_name] = {\n",
    "            'outer_scores': outer_scores,\n",
    "            'mean_score': np.mean(outer_scores),\n",
    "            'std_score': np.std(outer_scores),\n",
    "            'best_params_per_fold': best_params_per_fold\n",
    "        }\n",
    "        \n",
    "        print(f\"  Mean {scoring}: {np.mean(outer_scores):.4f} (+/- {np.std(outer_scores):.4f})\")\n",
    "        print(f\"  Individual fold scores: {[f'{score:.4f}' for score in outer_scores]}\")\n",
    "        print()\n",
    "    \n",
    "    return results\n",
    "\n",
    "def select_best_model_and_retrain(X, y, models_and_params, results, inner_cv=3, n_iter=50):\n",
    "    \"\"\"\n",
    "    Select the best model based on nested CV results and retrain on full dataset.\n",
    "    \"\"\"\n",
    "    # Find best model\n",
    "    best_model_name = max(results.keys(), key=lambda k: results[k]['mean_score'])\n",
    "    best_model_config = models_and_params[best_model_name]\n",
    "    \n",
    "    print(f\"=== BEST MODEL SELECTION ===\")\n",
    "    print(f\"Best model: {best_model_name}\")\n",
    "    print(f\"Expected performance: {results[best_model_name]['mean_score']:.4f} \"\n",
    "          f\"(+/- {results[best_model_name]['std_score']:.4f})\")\n",
    "    print()\n",
    "    \n",
    "    # Retrain best model on full dataset with hyperparameter tuning\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('select', SelectKBest(k=50)),\n",
    "        ('regressor', best_model_config['model'])\n",
    "    ])\n",
    "    \n",
    "    if best_model_config['params']:\n",
    "        inner_cv_splitter = KFold(n_splits=inner_cv, shuffle=True, random_state=42)\n",
    "        \n",
    "        final_search = RandomizedSearchCV(\n",
    "            pipeline,\n",
    "            best_model_config['params'],\n",
    "            cv=inner_cv_splitter,\n",
    "            scoring='r2',\n",
    "            n_iter=n_iter,\n",
    "            n_jobs=-1,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        final_search.fit(X, y)\n",
    "        final_model = final_search.best_estimator_\n",
    "        \n",
    "        print(f\"Final model hyperparameters: {final_search.best_params_}\")\n",
    "        print(f\"Cross-validation score on full dataset: {final_search.best_score_:.4f}\")\n",
    "    else:\n",
    "        pipeline.fit(X, y)\n",
    "        final_model = pipeline\n",
    "        print(\"No hyperparameters to tune for this model.\")\n",
    "    \n",
    "    return final_model, best_model_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Nested Cross-Validation for Airbnb Price Prediction...\n",
      "Training set shape: (17445, 55)\n",
      "Test set shape: (4362, 55)\n",
      "\n",
      "=== NESTED CROSS-VALIDATION FOR AIRBNB PRICE PREDICTION ===\n",
      "\n",
      "Dataset shape: (17445, 55)\n",
      "Target range: 10.000 - 500.000 (log-transformed)\n",
      "Outer CV: 5 folds, Inner CV: 3 folds\n",
      "Hyperparameter search iterations: 20\n",
      "\n",
      "Evaluating OLS...\n",
      "  Processing outer fold 1/5\n",
      "  Processing outer fold 2/5\n",
      "  Processing outer fold 3/5\n",
      "  Processing outer fold 4/5\n",
      "  Processing outer fold 5/5\n",
      "  Mean r2: 0.1502 (+/- 0.0105)\n",
      "  Individual fold scores: ['0.1378', '0.1405', '0.1624', '0.1625', '0.1478']\n",
      "\n",
      "Evaluating Ridge...\n",
      "  Processing outer fold 1/5\n",
      "  Processing outer fold 2/5\n",
      "  Processing outer fold 3/5\n",
      "  Processing outer fold 4/5\n",
      "  Processing outer fold 5/5\n",
      "  Mean r2: 0.1690 (+/- 0.0143)\n",
      "  Individual fold scores: ['0.1496', '0.1630', '0.1849', '0.1862', '0.1611']\n",
      "\n",
      "Evaluating ElasticNet...\n",
      "  Processing outer fold 1/5\n",
      "  Processing outer fold 2/5\n",
      "  Processing outer fold 3/5\n",
      "  Processing outer fold 4/5\n",
      "  Processing outer fold 5/5\n",
      "  Mean r2: 0.1573 (+/- 0.0109)\n",
      "  Individual fold scores: ['0.1421', '0.1511', '0.1710', '0.1686', '0.1539']\n",
      "\n",
      "Evaluating RandomForest...\n",
      "  Processing outer fold 1/5\n",
      "  Processing outer fold 2/5\n",
      "  Processing outer fold 3/5\n",
      "  Processing outer fold 4/5\n",
      "  Processing outer fold 5/5\n",
      "  Mean r2: 0.2420 (+/- 0.0251)\n",
      "  Individual fold scores: ['0.2037', '0.2256', '0.2716', '0.2648', '0.2445']\n",
      "\n",
      "Evaluating ExtraTrees...\n",
      "  Processing outer fold 1/5\n",
      "  Processing outer fold 2/5\n",
      "  Processing outer fold 3/5\n",
      "  Processing outer fold 4/5\n",
      "  Processing outer fold 5/5\n",
      "  Mean r2: 0.2369 (+/- 0.0257)\n",
      "  Individual fold scores: ['0.2066', '0.2168', '0.2768', '0.2547', '0.2294']\n",
      "\n",
      "Evaluating HistGB...\n",
      "  Processing outer fold 1/5\n",
      "  Processing outer fold 2/5\n",
      "  Processing outer fold 3/5\n",
      "  Processing outer fold 4/5\n",
      "  Processing outer fold 5/5\n",
      "  Mean r2: 0.2696 (+/- 0.0199)\n",
      "  Individual fold scores: ['0.2382', '0.2616', '0.2664', '0.2938', '0.2881']\n",
      "\n",
      "Evaluating GradientBoosting...\n",
      "  Processing outer fold 1/5\n",
      "  Processing outer fold 2/5\n",
      "  Processing outer fold 3/5\n",
      "  Processing outer fold 4/5\n",
      "  Processing outer fold 5/5\n",
      "  Mean r2: 0.2775 (+/- 0.0275)\n",
      "  Individual fold scores: ['0.2306', '0.2643', '0.3082', '0.2880', '0.2966']\n",
      "\n",
      "Evaluating KNN...\n",
      "  Processing outer fold 1/5\n",
      "  Processing outer fold 2/5\n",
      "  Processing outer fold 3/5\n",
      "  Processing outer fold 4/5\n",
      "  Processing outer fold 5/5\n",
      "  Mean r2: 0.1523 (+/- 0.0146)\n",
      "  Individual fold scores: ['0.1269', '0.1539', '0.1723', '0.1516', '0.1568']\n",
      "\n",
      "Evaluating SVR...\n",
      "  Processing outer fold 1/5\n",
      "  Processing outer fold 2/5\n",
      "  Processing outer fold 3/5\n",
      "  Processing outer fold 4/5\n",
      "  Processing outer fold 5/5\n",
      "  Mean r2: 0.1326 (+/- 0.0125)\n",
      "  Individual fold scores: ['0.1094', '0.1343', '0.1368', '0.1471', '0.1354']\n",
      "\n",
      "Evaluating XGBoost...\n",
      "  Processing outer fold 1/5\n",
      "  Processing outer fold 2/5\n",
      "  Processing outer fold 3/5\n",
      "  Processing outer fold 4/5\n",
      "  Processing outer fold 5/5\n",
      "  Mean r2: 0.2978 (+/- 0.0257)\n",
      "  Individual fold scores: ['0.2581', '0.2768', '0.3250', '0.3141', '0.3148']\n",
      "\n",
      "Evaluating CatBoost...\n",
      "  Processing outer fold 1/5\n",
      "  Processing outer fold 2/5\n",
      "  Processing outer fold 3/5\n",
      "  Processing outer fold 4/5\n",
      "  Processing outer fold 5/5\n",
      "  Mean r2: 0.2990 (+/- 0.0198)\n",
      "  Individual fold scores: ['0.2743', '0.2779', '0.3241', '0.3037', '0.3152']\n",
      "\n",
      "=== BEST MODEL SELECTION ===\n",
      "Best model: CatBoost\n",
      "Expected performance: 0.2990 (+/- 0.0198)\n",
      "\n",
      "Final model hyperparameters: {'regressor__depth': 7, 'regressor__iterations': 901, 'regressor__l2_leaf_reg': np.float64(2.1150972021685583), 'regressor__learning_rate': np.float64(0.03750796359625606), 'select__k': 111}\n",
      "Cross-validation score on full dataset: 0.2945\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Starting Nested Cross-Validation for Airbnb Price Prediction...\")\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "print()\n",
    "\n",
    "# Perform nested cross-validation (reduced subset for demonstration)\n",
    "# You can include all models by using the full models_and_params dictionary\n",
    "selected_models = models_and_params\n",
    "\n",
    "\n",
    "cv_results = nested_cross_validation_regression(\n",
    "    X_train, y_train, \n",
    "    selected_models,  # Use selected_models or models_and_params for all\n",
    "    outer_cv=5, \n",
    "    inner_cv=3, \n",
    "    n_iter=20,  # Reduced for faster execution\n",
    "    scoring='r2'\n",
    ")\n",
    "\n",
    "# Select and retrain best model\n",
    "final_model, best_model_name = select_best_model_and_retrain(\n",
    "    X_train, y_train, selected_models, cv_results, n_iter=30\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FINAL EVALUATION ON TEST SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== FINAL TEST SET EVALUATION ===\n",
      "Test R² (log scale): -28.0738\n",
      "Test RMSE (log scale): 3.1668\n",
      "Test MAE (log scale): 3.1119\n",
      "\n",
      "In original price scale:\n",
      "Test RMSE: $942.61\n",
      "Test MAE: $941.25\n",
      "\n",
      "=== NESTED CV SUMMARY ===\n",
      "               Model  Mean_R2  Std_R2  CI_Lower  CI_Upper\n",
      "10          CatBoost   0.2990  0.0198    0.2601    0.3379\n",
      "9            XGBoost   0.2978  0.0257    0.2473    0.3482\n",
      "6   GradientBoosting   0.2775  0.0275    0.2236    0.3315\n",
      "5             HistGB   0.2696  0.0199    0.2306    0.3087\n",
      "3       RandomForest   0.2420  0.0251    0.1929    0.2912\n",
      "4         ExtraTrees   0.2369  0.0257    0.1866    0.2871\n",
      "1              Ridge   0.1690  0.0143    0.1409    0.1970\n",
      "2         ElasticNet   0.1573  0.0109    0.1359    0.1787\n",
      "7                KNN   0.1523  0.0146    0.1237    0.1809\n",
      "0                OLS   0.1502  0.0105    0.1296    0.1708\n",
      "8                SVR   0.1326  0.0125    0.1082    0.1570\n",
      "\n",
      "Selected model: CatBoost\n",
      "Note: The nested CV performance estimates represent unbiased estimates\n",
      "of how well each model is expected to perform on unseen data.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== FINAL TEST SET EVALUATION ===\")\n",
    "y_pred_test = final_model.predict(X_test)\n",
    "\n",
    "# Clip predictions in log scale to avoid extreme values\n",
    "y_pred_test_clipped = np.clip(y_pred_test, 0, 7)  # log(1000) ≈ 7, assuming no price > $1000 per person\n",
    "\n",
    "# Calculate metrics in log scale\n",
    "test_r2 = r2_score(y_test_log, y_pred_test_clipped)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test_log, y_pred_test_clipped))\n",
    "test_mae = mean_absolute_error(y_test_log, y_pred_test_clipped)\n",
    "\n",
    "print(f\"Test R² (log scale): {test_r2:.4f}\")\n",
    "print(f\"Test RMSE (log scale): {test_rmse:.4f}\")\n",
    "print(f\"Test MAE (log scale): {test_mae:.4f}\")\n",
    "\n",
    "# Convert back from log scale and clip to reasonable values\n",
    "y_pred_original = np.exp(y_pred_test_clipped) - 1\n",
    "y_pred_original = np.clip(y_pred_original, 0, 1000)  # Max $1000 per person\n",
    "\n",
    "# Convert test values back and clip them as well\n",
    "y_test_actual = y_test.values  # Original non-log values\n",
    "\n",
    "# Calculate metrics in original scale\n",
    "test_rmse_original = np.sqrt(mean_squared_error(y_test_actual, y_pred_original))\n",
    "test_mae_original = mean_absolute_error(y_test_actual, y_pred_original)\n",
    "\n",
    "print(f\"\\nIn original price scale:\")\n",
    "print(f\"Test RMSE: ${test_rmse_original:.2f}\")\n",
    "print(f\"Test MAE: ${test_mae_original:.2f}\")\n",
    "\n",
    "# Summary of all models\n",
    "print(\"\\n=== NESTED CV SUMMARY ===\")\n",
    "cv_summary = pd.DataFrame([\n",
    "    {\n",
    "        'Model': model_name,\n",
    "        'Mean_R2': result['mean_score'],\n",
    "        'Std_R2': result['std_score'],\n",
    "        'CI_Lower': result['mean_score'] - 1.96 * result['std_score'],\n",
    "        'CI_Upper': result['mean_score'] + 1.96 * result['std_score']\n",
    "    }\n",
    "    for model_name, result in cv_results.items()\n",
    "]).sort_values('Mean_R2', ascending=False)\n",
    "\n",
    "print(cv_summary.round(4))\n",
    "\n",
    "print(f\"\\nSelected model: {best_model_name}\")\n",
    "print(\"Note: The nested CV performance estimates represent unbiased estimates\")\n",
    "print(\"of how well each model is expected to perform on unseen data.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
